{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sampathn2005/google-ai-studio-text-gen/blob/main/Gemini_2_0_Flash_Thinking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini 2.0 Flash Thinking"
      ],
      "metadata": {
        "id": "rMwmspDF1QxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemini 2.0 Flash Thinking model is an experimental model that's trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses than the Gemini 2.0 Flash Experimental model.\n",
        "\n"
      ],
      "metadata": {
        "id": "WfVnlsUz1T9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use thinking models"
      ],
      "metadata": {
        "id": "KMiGFmL71hfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flash Thinking models are available in Google AI Studio and through the Gemini API. The Gemini API doesn't return thoughts in the response."
      ],
      "metadata": {
        "id": "bwRmb6PO1V1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We have set up gemini-2.0-flash-thinking-exp as an alias to the latest Flash Thinking model. Use this alias to get the latest Flash thinking model, or specify the full model name."
      ],
      "metadata": {
        "id": "lGDJ3B-R1oMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send a basic request"
      ],
      "metadata": {
        "id": "m5rJ2JKQ1sXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example uses the new Google Genai SDK and the v1alpha version of the API."
      ],
      "metadata": {
        "id": "FLIbwL-C2R2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNgLHxf_0BVc",
        "outputId": "bd05fe5a-84b7-49b4-86da-b3bcadc63731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's break down RLHF (Reinforcement Learning from Human Feedback) in super simple terms.\n",
            "\n",
            "Imagine you have a really smart student (the AI language model) who has read a huge amount of books (its initial training data). It's great at writing sentences and continuing stories, but it doesn't really know what kind of answers you *personally* find helpful, honest, or safe. It just knows how to predict the next word based on what it's seen.\n",
            "\n",
            "RLHF is like giving this smart student a **personal coach** who represents human preferences. Here's how it works in three main steps:\n",
            "\n",
            "1.  **Getting Human Opinions (The Coach's Taste):**\n",
            "    *   You show humans different answers the AI gave for the *same question*.\n",
            "    *   Humans **rank** these answers from best to worst. They say things like, \"I like answer C the best, then A, then B, then D.\"\n",
            "    *   You collect a lot of these human rankings. This teaches you what humans generally *prefer*.\n",
            "\n",
            "2.  **Building a \"Preference Judge\" (Training the Coach):**\n",
            "    *   You train a *separate, smaller AI model* to act like a \"preference judge.\"\n",
            "    *   You feed it the data from Step 1 (which answers humans liked more than others).\n",
            "    *   This judge AI learns to predict how a human would rank *any* given answer. It can now give a \"score\" to any text the main AI generates, estimating how much a human would like it.\n",
            "\n",
            "3.  **Teaching the Main AI Using the Judge (The Student Learning from the Coach):**\n",
            "    *   Now, you take the main language model (the smart student).\n",
            "    *   You use a technique called \"Reinforcement Learning.\" Think of this like training through trial and error, getting points for good actions.\n",
            "    *   The main AI generates an answer.\n",
            "    *   The \"preference judge\" AI from Step 2 gives this answer a \"score\" (like points in a game), based on how much it *thinks* a human would like it.\n",
            "    *   The main AI then adjusts how it generates text to try and get *higher* scores from the judge AI in the future. It's learning to create answers that the judge (which represents human preferences) will like.\n",
            "\n",
            "**In essence:**\n",
            "\n",
            "RLHF uses human feedback to train a separate model that *mimics* human preferences. Then, the main AI model is trained to maximize the \"score\" given by this human-mimicking model, making it better at generating text that humans are likely to find helpful, honest, and harmless, rather than just statistically probable.\n",
            "\n",
            "It's like teaching the AI not just *how* to talk, but *what* kind of things humans prefer it to say.\n"
          ]
        }
      ],
      "source": [
        "GEMINI_API_KEY = 'AIzaSyA_YOH0-UZl34pNjrh_TPrWSLSmhqvgQGA'\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key= GEMINI_API_KEY, http_options={'api_version':'v1alpha'})\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        "    contents='Explain how RLHF works in simple terms.',\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-turn thinking conversations"
      ],
      "metadata": {
        "id": "eDgB2Khq2CKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During multi-turn conversations, you pass the entire conversation history as input, so the model has access to its previous thoughts in a multi-turn conversation."
      ],
      "metadata": {
        "id": "0RYC65KT2FiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new Google Genai SDK provides the ability to create a multi-turn chat session which is helpful to manage the state of a conversation.\n",
        "\n"
      ],
      "metadata": {
        "id": "qvYJh-HA229c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key= GEMINI_API_KEY, http_options={'api_version':'v1alpha'})\n",
        "\n",
        "chat = client.aio.chats.create(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        ")\n",
        "response = await chat.send_message('What is your name?')\n",
        "print(response.text)\n",
        "response = await chat.send_message('What did you just say before this?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRGtn3mP2Exk",
        "outputId": "9f19f796-3204-4635-de4f-9c0518c634f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do not have a name. I am a large language model, trained by Google.\n",
            "Before you asked \"What did you just say before this?\", I said:\n",
            "\n",
            "\"I do not have a name. I am a large language model, trained by Google.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations"
      ],
      "metadata": {
        "id": "YhRukiLx3aWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Flash Thinking model is an experimental model and has the following limitations:\n",
        "\n",
        "*   No JSON mode or Search Grounding\n",
        "*   Thoughts are only shown in Google AI Studio\n"
      ],
      "metadata": {
        "id": "IKfAQOah3fAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's next?"
      ],
      "metadata": {
        "id": "8k5KEThb3i_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Try the Flash Thinking model in Google AI Studio.\n",
        "*   Try the Flash Thinking Colab\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKEgy63q3mN3"
      }
    }
  ]
}